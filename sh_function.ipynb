{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9586bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1be30d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "#SONG_ID = 8\n",
    "SR = 44100\n",
    "FRAME_SIZE = 1024\n",
    "HOP_SIZE = 512\n",
    "\n",
    "SONG_LOCATION = \"query/countryroads.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7723c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_song(song_location):\n",
    "    y, sr = librosa.load(song_location, sr=SR)  \n",
    "\n",
    "    print(f\"Shape: {y.shape}, Sample Rate: {SR}\")\n",
    "\n",
    "    time = np.linspace(0, len(y)/sr, len(y)) # start, stop, no of points\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.plot(time, y, color='blue')\n",
    "    # plt.show()\n",
    "\n",
    "    return spectro(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d22df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectro(y):\n",
    "    D = librosa.stft(y, n_fft = 2048, hop_length=HOP_SIZE)\n",
    "\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # librosa.display.specshow(S_db, sr=SR, hop_length=512,\n",
    "    #                         x_axis='time', y_axis='hz', cmap='magma')\n",
    "    # plt.colorbar(format=\"%+2.0f dB\")\n",
    "    # plt.title(\"Spectrogram (dB)\")\n",
    "    # plt.show()\n",
    "\n",
    "    return find_peaks(S_db, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import maximum_filter, generate_binary_structure, iterate_structure\n",
    "\n",
    "def find_peaks(spec_db, y):\n",
    "\n",
    "    from scipy.ndimage import maximum_filter\n",
    "    neighborhood_size = (25, 25)\n",
    "    local_max = maximum_filter(spec_db, size=neighborhood_size) == spec_db\n",
    "    peaks = np.argwhere(local_max)\n",
    "\n",
    "    # filtering out the low frequencies\n",
    "    peaks = [(t, f) for f, t in peaks if spec_db[f, t] > -40]  # -40 dB threshold\n",
    "\n",
    "    peak_points = []\n",
    "    for (t, f) in peaks:\n",
    "        time = t * HOP_SIZE / SR\n",
    "        freq = f * SR / 2048  # changed this from FRAME_SIZE -> n_fft\n",
    "        peak_points.append((freq, time))\n",
    "\n",
    "    #plot_peaks(y,spec_db, peak_points)    \n",
    "\n",
    "    return peak_points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2bfeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def plot_peaks(y, spec_db, peak_points):    \n",
    "    # plot peaks\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot spectrogram\n",
    "    plt.imshow(spec_db, origin=\"lower\", aspect=\"auto\", cmap=\"magma\",\n",
    "            extent=[0, len(y)/SR, 0, SR/2])\n",
    "\n",
    "    # Overlay peaks\n",
    "    freqs = [p[0] for p in peak_points]\n",
    "    times = [p[1] for p in peak_points]\n",
    "    plt.scatter(times, freqs, color=\"cyan\", marker=\".\", s=10, label=\"Peaks\")\n",
    "\n",
    "    plt.colorbar(label=\"Magnitude (dB)\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    plt.title(\"Spectrogram with Detected Peaks\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1055af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodehash(f_a, f_b, delta_t_frames):\n",
    "    # f_a, f_b: frequency bin indices (int)\n",
    "    # delta_t_frames: integer difference in frames (anchor->target)\n",
    "    f_a = int(f_a) & ((1 << 11) - 1)\n",
    "    f_b = int(f_b) & ((1 << 11) - 1)\n",
    "    dt  = int(delta_t_frames) & ((1 << 10) - 1)\n",
    "\n",
    "    hash_val = (f_a << (11 + 10)) | (f_b << 10) | dt\n",
    "    return hash_val & 0xFFFFFFFF\n",
    "\n",
    "def decodehash(hash_val):\n",
    "    dt  = hash_val & ((1 << 10) - 1)\n",
    "    f_b = (hash_val >> 10) & ((1 << 11) - 1)\n",
    "    f_a = (hash_val >> (10 + 11)) & ((1 << 11) - 1)\n",
    "    return f_a, f_b, dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "811d08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "# These constants define the bit allocation for packing the hash\n",
    "FREQ_BITS = 10 \n",
    "TIME_BITS = 8\n",
    "HASH_BITS = FREQ_BITS * 2 + TIME_BITS  # Total bits for the hash\n",
    "\n",
    "# Hashing function using a more robust bit packing method\n",
    "def encode_hash(f1, f2, delta_t):\n",
    "    \"\"\"\n",
    "    Encodes two frequencies and their time difference into a single hash.\n",
    "    Uses bit manipulation to pack the information efficiently.\n",
    "    \"\"\"\n",
    "    # Use hashlib for a more robust initial hash\n",
    "    # This combines the three values into a single hashable string\n",
    "    key = f\"{int(f1)}-{int(f2)}-{int(delta_t)}\".encode('utf-8')\n",
    "    \n",
    "    # Use SHA-1, then take a portion of its output for our fingerprint\n",
    "    # This creates a highly unique integer from the key\n",
    "    h = hashlib.sha1(key)\n",
    "    \n",
    "    # Take the first few bytes and convert to an integer\n",
    "    # We use HASH_BITS to ensure the hash fits within our defined size\n",
    "    return int(h.hexdigest()[:HASH_BITS // 4], 16)\n",
    "\n",
    "\n",
    "def create_hashes(peaks, song_id):\n",
    "    \"\"\"\n",
    "    Generates a dictionary of hashes from a list of spectral peaks.\n",
    "    A hash is created from a pair of peaks (an anchor and a target).\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary where keys are the hashes and values are lists of \n",
    "        tuples: (song_id, anchor_peak_time).\n",
    "    \"\"\"\n",
    "    hashes = {}\n",
    "\n",
    "    peaks.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # These parameters define the \"target zone\" for pairing peaks\n",
    "    MIN_TIME_DELTA = 0.5  # Minimum time offset in seconds\n",
    "    MAX_TIME_DELTA = 2.0  # Maximum time offset in seconds\n",
    "    FAN_OUT = 15          # Max number of peaks to pair with an anchor\n",
    "\n",
    "    total_peaks = len(peaks)\n",
    "    for i in range(total_peaks):\n",
    "        f1, t1 = peaks[i]\n",
    "        \n",
    "        pairs_formed = 0\n",
    "        for j in range(i + 1, total_peaks):\n",
    "            f2, t2 = peaks[j]\n",
    "            delta_t = t2 - t1\n",
    "\n",
    "            # Check if the time difference is within our target window\n",
    "            if MIN_TIME_DELTA <= delta_t <= MAX_TIME_DELTA:\n",
    "                # Create a hash from the frequencies and time delta\n",
    "                hash_val = encode_hash(f1, f2, delta_t)\n",
    "\n",
    "                # Store the hash with the song_id and the ANCHOR's absolute time\n",
    "                if hash_val not in hashes:\n",
    "                    hashes[hash_val] = []\n",
    "                hashes[hash_val].append((song_id, t1))\n",
    "                \n",
    "                pairs_formed += 1\n",
    "                if pairs_formed >= FAN_OUT:\n",
    "                    break\n",
    "            \n",
    "            # Optimization: If we've passed the max time delta, move to the next anchor\n",
    "            elif delta_t > MAX_TIME_DELTA:\n",
    "                break\n",
    "                \n",
    "    return hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca8e01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to local database using pickle for now\n",
    "# We can move onto using a better database like redis later\n",
    "import pickle\n",
    "DB_FILENAME = \"fingerprints.pkl\"\n",
    "\n",
    "def save_hashes(hashes):\n",
    "    with open(DB_FILENAME, 'ab') as f:  \n",
    "        pickle.dump(hashes, f)\n",
    "    print(f\"Successfully appended {len(hashes)} fingerprints to {DB_FILENAME}\")\n",
    "\n",
    "def load_hashes():\n",
    "    with open(DB_FILENAME, 'rb') as f:\n",
    "        loaded_hashes = pickle.load(f)\n",
    "    \n",
    "    return loaded_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a010d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pickle\n",
    "\n",
    "def load_hashes():\n",
    "    \"\"\"Loads the fingerprint database from a pickle file.\"\"\"\n",
    "    with open('fingerprints.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# 'hashes' is the variable you generated from fingerprinting your query song.\n",
    "# Its structure is {hash: [(song_id, time), ...]}\n",
    "# Example: {4723200: [(4, np.float64(2.5))]}\n",
    "\n",
    "def match_hashes(hashes):\n",
    "    # 1. Load the database of saved song fingerprints\n",
    "    saved_hashes = load_hashes()\n",
    "\n",
    "    # 2. Run the matching algorithm directly on the query's hashes\n",
    "    histogram = collections.defaultdict(int)\n",
    "    for qhash, t_q_frames in hashes.items():\n",
    "        if qhash in saved_hashes:\n",
    "            for db_song_id, t_db_frames in saved_hashes[qhash]:\n",
    "                offset_frames = int(t_db_frames - t_q_frames)\n",
    "                key = (int(db_song_id), offset_frames)\n",
    "                histogram[key] += 1\n",
    "\n",
    "\n",
    "    # 3. Find the song with the most matching offsets\n",
    "    if not histogram:\n",
    "        print(\"No matches found.\")\n",
    "    else:\n",
    "        # Find the (song_id, offset) pair with the highest vote count\n",
    "        best_match = max(histogram.items(), key=lambda item: item[1])\n",
    "        \n",
    "        (song_id, offset), num_votes = best_match\n",
    "\n",
    "        print(\"--- Match Found! ---\")\n",
    "        print(f\"Best Match: Song ID {song_id}\")\n",
    "        print(f\"Confidence (votes): {num_votes}\")\n",
    "\n",
    "        # Optional: You can add your debugging block here if needed\n",
    "        print(\"\\n--- Debug Info ---\")\n",
    "        query_hashes_set = set(hashes.keys())\n",
    "        db_hashes_set = set(saved_hashes.keys())\n",
    "        common_hashes = query_hashes_set.intersection(db_hashes_set)\n",
    "        print(f\"Total Hashes in Query: {len(query_hashes_set)}\")\n",
    "        print(f\"Total Hashes in Database: {len(db_hashes_set)}\")\n",
    "        print(f\"Number of Common Hashes Found: {len(common_hashes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b6d507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for query\n",
    "#match_hashes(hashes)\n",
    "\n",
    "def find_match(SONG_LOCATION, SONG_ID):\n",
    "    peaks = load_song(SONG_LOCATION)\n",
    "    hashes = create_hashes(peaks, SONG_ID)\n",
    "\n",
    "    match_hashes(hashes)\n",
    "\n",
    "# find_match(\"query/countryroads.mp3\", 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6fe5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pickle\n",
    "import collections\n",
    "from scipy.ndimage import maximum_filter\n",
    "from scipy.ndimage import generate_binary_structure\n",
    "\n",
    "def save_db():\n",
    "    \"\"\"Builds and saves the fingerprint database.\"\"\"\n",
    "    songs = [\n",
    "        (\"songs/Hangman.mp3\", 0),\n",
    "        (\"songs/The Moon.mp3\", 1),\n",
    "        (\"songs/02. Eleanor Rigby.mp3\", 2),\n",
    "        (\"songs/03. I'm Only Sleeping.mp3\",3)\n",
    "    ]\n",
    "    \n",
    "    db = collections.defaultdict(list)\n",
    "    for location, song_id in songs:\n",
    "        print(f\"Fingerprinting '{location}' with SONG_ID = {song_id}\")\n",
    "        peaks = load_song(location)\n",
    "        print(peaks[:5])\n",
    "        hashes = create_hashes(peaks, song_id)\n",
    "        for hash_val, hash_info in hashes.items():\n",
    "            db[hash_val].extend(hash_info)\n",
    "            \n",
    "    with open('fingerprints.pkl', 'wb') as f:\n",
    "        pickle.dump(db, f)\n",
    "    print(\"Database saved successfully.\")\n",
    "\n",
    "def fingerprint_query(song_location):\n",
    "    peaks = load_song(song_location)\n",
    "    hashes_with_id = create_hashes(peaks, song_id=-1)\n",
    "    query_hashes = {}\n",
    "    for h, lst in hashes_with_id.items():\n",
    "        if lst:\n",
    "            query_hashes[h] = lst[0][1]\n",
    "    return query_hashes\n",
    "\n",
    "# THE CORRECTED MATCHING FUNCTION\n",
    "def find_match(song_location):\n",
    "    \"\"\"Finds the best match for a query song against the database.\"\"\"\n",
    "    \n",
    "    # 1. Load the database\n",
    "    with open('fingerprints.pkl', 'rb') as f:\n",
    "        saved_hashes = pickle.load(f)\n",
    "        \n",
    "    # 2. Fingerprint the query song (without assigning a real ID)\n",
    "    query_hashes = fingerprint_query(song_location)\n",
    "    \n",
    "    # 3. Run the matching algorithm\n",
    "    histogram = collections.defaultdict(int)\n",
    "    for qhash, t_q_frame in query_hashes.items():\n",
    "        if qhash in saved_hashes:\n",
    "            for db_song_id, t_db_frame in saved_hashes[qhash]:\n",
    "                offset_frames = int(t_db_frame - t_q_frame)\n",
    "                histogram[(int(db_song_id), offset_frames)] += 1\n",
    "                \n",
    "    # 4. Find the winning song\n",
    "    if not histogram:\n",
    "        print(\"No matches found.\")\n",
    "        return None\n",
    "    \n",
    "    song_votes = collections.defaultdict(int)\n",
    "    for (song_id, offset), votes in histogram.items():\n",
    "        song_votes[song_id] += votes\n",
    "\n",
    "    if not song_votes:\n",
    "        print(\"No matches found.\")\n",
    "        return None\n",
    "\n",
    "    sorted_songs = sorted(song_votes.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    best_song_id, best_song_votes = sorted_songs[0]\n",
    "\n",
    "    if len(sorted_songs) > 1:\n",
    "        second_best_votes = sorted_songs[1][1]\n",
    "        confidence = (1 - (second_best_votes / best_song_votes)) * 100\n",
    "        print(f\"Confidence: {confidence:.2f}% (Winner has {best_song_votes} votes, runner-up has {second_best_votes} votes)\")\n",
    "        print(f\"Best song: Song ID {best_song_id}\")\n",
    "    else:\n",
    "        confidence = 100.0\n",
    "        print(f\"Confidence: 100% (Only one song found with {best_song_votes} votes)\")\n",
    "\n",
    "    # You can now set a threshold on this confidence percentage\n",
    "    if confidence < 75: # Example threshold\n",
    "        print(\"Match found, but confidence is too low.\")\n",
    "        return None\n",
    "\n",
    "    # Continue with reporting the best_match from the original histogram\n",
    "    print(f\"Best Match: Song ID {best_song_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "686db5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fingerprinting 'songs/Hangman.mp3' with SONG_ID = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (8649728,), Sample Rate: 44100\n",
      "[(np.float64(107.666015625), np.float64(186.8974149659864)), (np.float64(150.732421875), np.float64(67.29142857142857)), (np.float64(150.732421875), np.float64(111.68798185941043)), (np.float64(150.732421875), np.float64(179.79210884353742)), (np.float64(172.265625), np.float64(21.97768707482993))]\n",
      "Fingerprinting 'songs/The Moon.mp3' with SONG_ID = 1\n",
      "Shape: (13965312,), Sample Rate: 44100\n",
      "[(np.float64(43.06640625), np.float64(205.49659863945578)), (np.float64(43.06640625), np.float64(208.70095238095237)), (np.float64(43.06640625), np.float64(300.36172335600907)), (np.float64(43.06640625), np.float64(302.63727891156464)), (np.float64(43.06640625), np.float64(306.2363718820862))]\n",
      "Fingerprinting 'songs/02. Eleanor Rigby.mp3' with SONG_ID = 2\n",
      "Shape: (5574656,), Sample Rate: 44100\n",
      "[(np.float64(86.1328125), np.float64(43.17750566893424)), (np.float64(86.1328125), np.float64(43.35165532879819)), (np.float64(107.666015625), np.float64(53.9631746031746)), (np.float64(107.666015625), np.float64(62.61260770975057)), (np.float64(107.666015625), np.float64(108.91319727891157))]\n",
      "Fingerprinting 'songs/03. I'm Only Sleeping.mp3' with SONG_ID = 3\n",
      "Shape: (7974912,), Sample Rate: 44100\n",
      "[(np.float64(64.599609375), np.float64(6.141678004535147)), (np.float64(64.599609375), np.float64(6.397097505668934)), (np.float64(64.599609375), np.float64(8.161814058956915)), (np.float64(64.599609375), np.float64(8.452063492063493)), (np.float64(64.599609375), np.float64(8.6378231292517))]\n",
      "Database saved successfully.\n"
     ]
    }
   ],
   "source": [
    "save_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66a67dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x2c302c39 at offset 1203840.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "/var/folders/wp/vt3nky3n4yj60swtpg7yz1500000gn/T/ipykernel_70028/615281672.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(song_location, sr=SR)\n",
      "/Users/anantagarwal/Desktop/dev/projects/shazam/.venv/lib/python3.13/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1327234,), Sample Rate: 44100\n",
      "Confidence: 83.14% (Winner has 14281 votes, runner-up has 2408 votes)\n",
      "Best song: Song ID 0\n",
      "Best Match: Song ID 0\n"
     ]
    }
   ],
   "source": [
    "find_match(\"query/hangman.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ede70fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1345227,), Sample Rate: 44100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x2c332c31 at offset 1220160.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "/var/folders/wp/vt3nky3n4yj60swtpg7yz1500000gn/T/ipykernel_70028/615281672.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(song_location, sr=SR)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence: 24.71% (Winner has 7081 votes, runner-up has 5331 votes)\n",
      "Best song: Song ID 2\n",
      "Match found, but confidence is too low.\n"
     ]
    }
   ],
   "source": [
    "find_match(\"query/rigby.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "535fa581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 63319, 1: 253720, 2: 59420}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def count_song_ids(pkl_path, song_ids=(0,1,2)):\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        saved_hashes = pickle.load(f)\n",
    "\n",
    "    counter = Counter()\n",
    "    for hash_val, entries in saved_hashes.items():\n",
    "        for song_id, t in entries:\n",
    "            if song_id in song_ids:\n",
    "                counter[song_id] += 1\n",
    "\n",
    "    # return results in consistent order\n",
    "    return {sid: counter.get(sid, 0) for sid in song_ids}\n",
    "\n",
    "counts = count_song_ids(\"fingerprints.pkl\", song_ids=(0,1,2))\n",
    "print(counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5139536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1345227,), Sample Rate: 44100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x2c302c33 at offset 1220160.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "/var/folders/wp/vt3nky3n4yj60swtpg7yz1500000gn/T/ipykernel_70028/615281672.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(song_location, sr=SR)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence: 82.17% (Winner has 12709 votes, runner-up has 2266 votes)\n",
      "Best song: Song ID 1\n",
      "Best Match: Song ID 1\n"
     ]
    }
   ],
   "source": [
    "find_match(\"query/moon.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bceef68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1345227,), Sample Rate: 44100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x2c332c31 at offset 1220160.\n",
      "Note: Trying to resync...\n",
      "/var/folders/wp/vt3nky3n4yj60swtpg7yz1500000gn/T/ipykernel_70028/615281672.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(song_location, sr=SR)\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence: 24.71% (Winner has 7081 votes, runner-up has 5331 votes)\n",
      "Best song: Song ID 2\n",
      "Match found, but confidence is too low.\n"
     ]
    }
   ],
   "source": [
    "find_match(\"query/rigby.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c346c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Illegal Audio-MPEG-Header 0x2c302c31 at offset 1198080.\n",
      "Note: Trying to resync...\n",
      "Note: Skipped 1024 bytes in input.\n",
      "[src/libmpg123/parse.c:wetwork():1349] error: Giving up resync after 1024 bytes - your stream is not nice... (maybe increasing resync limit could help).\n",
      "/var/folders/wp/vt3nky3n4yj60swtpg7yz1500000gn/T/ipykernel_70028/615281672.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(song_location, sr=SR)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1320884,), Sample Rate: 44100\n",
      "Confidence: 14.87% (Winner has 5601 votes, runner-up has 4768 votes)\n",
      "Best song: Song ID 1\n",
      "Match found, but confidence is too low.\n"
     ]
    }
   ],
   "source": [
    "find_match(\"query/sleeping.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bcb072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
